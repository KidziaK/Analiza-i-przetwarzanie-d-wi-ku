M1.SSE <- sum(M1$residuals^2)
F <- ((M1.SSE - M0.SSE) / (p - q)) / (M0.SSE / (n - p))
interaction.plot(df$day,df$section,df$response)
#dużo przecięć, a więc najprawdopodobniej interakcje są znaczące
p_value <- pf(F, p - q, n - p, lower.tail = FALSE) #1.124687e-07
#małe p-value, a więc model iterakcje są istotne
p_value
# Wczytaj dane
df <- read.csv("newspaper_ML.csv")
df <- df[,-1] # Usuń pierwszą kolumnę
# a)
M0 <- lm(response ~ I(day*section), data = df)
M1 <- lm(response ~ day + section, data = df)
n = nrow(df)
p = length(M0$coefficients)
q = length(M1$coefficients)
# Policz SSE dla M0 i M1
SSE_M0 <- sum(M0$residuals^2)
SSE_M1 <- sum(M1$residuals^2)
F <- ((SSE_M1 - SSE_M0) / (p - q)) / (SSE_M0 / (n - p))
interaction.plot(df$day,df$section,df$response)
# Na wykresie widać dużo przecięć, więc możemy stwierdzić,
# że interakcje są istotne
p_val <- pf(F, p - q, n - p, lower.tail = FALSE)
# p-value na poziomie 0.1, co sugeruje, że interakcje nie są istotne
p_val
summary(M0)
summary(M1)
interaction.plot(df$section,df$response)
M1_day <- lm(response~day, data = df)
M1_section <- lm(response~section, data = df)
anova(M1,M1_day)
anova(M1,M1_section)
# Oba czynniki <0.005, zatem są istotne,
interaction.plot(df$section,df$response)
M2 <- lm(response~day, data = df)
interaction.plot(df$section,df$response)
interaction.plot(df$section,df$response)
df$section
df$response
# a)
M0 <- lm(response~day*section, data = df)
M1 <- lm(response~day+section, data = df)
p = length(M0$coefficients)
q = length(M1$coefficients)
n = nrow(df)
M0.SSE <- sum(M0$residuals^2)
M1.SSE <- sum(M1$residuals^2)
F <- ((M1.SSE - M0.SSE) / (p - q)) / (M0.SSE / (n - p))
interaction.plot(df$day,df$section,df$response)
#dużo przecięć, a więc najprawdopodobniej interakcje są znaczące
p_value <- pf(F, p - q, n - p, lower.tail = FALSE) #1.124687e-07
#małe p-value, a więc model iterakcje są istotne
# b)
summary(M0)
summary(M1)
# ref: Friday:Business
# M0: 9 + 2.5 = 11.5
# M1: 11.683 -2.833 = 8.85
# c)
M1_day <- lm(response~day, data = df)
M1_section <- lm(response~section, data = df)
anova(M1,M1_day) #0.002692
anova(M1,M1_section) #1.187e-05
#obydwa czynnki są istotne, ale dzień jest bradziej istotny
#można to także sprawdzać na wykresie z interakcjami
interaction.plot(df$section,df$response)
seq(0,5,0.05)
seq(0,5,0.005)
length(seq(0,5,0.005))
length(seq(0,5,1/n))
length(seq(0,5,1/p))
length(seq(0,5,0.05))
length(seq(0,5,0.005))
length(seq(0.005,5,0.005))
# OĹ›wiadczam, ĹĽe niniejsza praca stanowiÄ…ca podstwÄ™ do uzania osiÄ…gnieÄ‡ia efektĂłw
# uczenia siÄ™ z przedmiotu Stosowana Analiza Regresji zostaĹ‚a wykonana samodzielnie.
# MikoĹ‚aj Kida 276944
# Kolokwium 2
library(MASS)
library(glmnet)
library(dplyr)
#______________________________________________________________________________
# Zadanie 1)
# a)
p <- 200
n <- 1000
sigma <- 1
# Wygeneruj pró
p <- 200
n <- 1000
sigma <- 1
CovMatrix <- matrix(0.5, p, p) + 0.5*diag(p)
# Wygeneruj próbkę
x <- mvrnorm(n, mu = seq(0.005,5,0.005), Sigma = CovMatrix)
eps <- rnorm(n, 0, sigma)
beta <- c(rep(1, 10), rep(0, 90))
y <- x %*% beta + eps
dim(x)
# Wygeneruj próbkę
x <- mvrnorm(n, mu = seq(0.005,5,0.01), Sigma = CovMatrix)
# Wygeneruj próbkę
x <- mvrnorm(n, mu = seq(0.01,5,0.01), Sigma = CovMatrix)
seq(0.01,5,0.01)
length(seq(0.01,5,0.01))
length(seq(0.01,5,0.05))
length(seq(0.01,5,0.025))
# Wygeneruj próbkę
x <- mvrnorm(n, mu = seq(0.01,5,0.025), Sigma = CovMatrix)
# Zadanie 1)
# a)
p <- 200
n <- 1000
sigma <- 1
CovMatrix <- matrix(0.5, p, p) + 0.5*diag(p)
# Wygeneruj próbkę
x <- mvrnorm(n, mu = seq(0.01,5,0.025), Sigma = CovMatrix)
eps <- rnorm(n, 0, sigma)
beta <- c(rep(1, 10), rep(0, 90))
y <- x %*% beta + eps
p <- 200
n <- 1000
sigma <- 1
CovMatrix <- matrix(0.5, p, p) + 0.5*diag(p)
# Wygeneruj próbkę
x <- mvrnorm(n, mu = seq(0.01,5,0.025), Sigma = CovMatrix)
eps <- rnorm(n, 0, sigma)
beta <- c(rep(1, 10), rep(0, 190))
y <- x %*% beta + eps
# Utwórz ramkę danych
# MikoĹ‚aj Kida 276944
# Kolokwium 2
library(MASS)
library(glmnet)
library(dplyr)
#______________________________________________________________________________
# Zadanie 1)
# a)
p <- 200
n <- 1000
sigma <- 1
CovMatrix <- matrix(0.5, p, p) + 0.5*diag(p)
# Wygeneruj próbkę
x <- mvrnorm(n, mu = seq(0.01,5,0.025), Sigma = CovMatrix)
eps <- rnorm(n, 0, sigma)
beta <- c(rep(1, 10), rep(0, 190))
y <- x %*% beta + eps
# Utwórz ramkę danych
dane <- data.frame(y, x)
# Podziel dane na podzbiory
rows <- sample(nrow(dane),n/2)
treningowy <- dane[rows,]
walidacyjny <- dane[-rows,]
# c)
# Dopasuj model na zbiorze treningowym
cv_model <- cv.glmnet(x = data.matrix(treningowy[,-1]), y = treningowy[,1], alpha = 0.5)
# d)
# Optymalny lambda
cv_model$lambda.min
p <- 200
n <- 1000
sigma <- 1
CovMatrix <- diag(p)
for (i in 1:p) {
m <- matrix(0, p, p)
m[abs(row(m) - col(m)) == i] <- (0.5)^i
CovMatrix <- CovMatrix + m
}
p <- 4
n <- 1000
sigma <- 1
CovMatrix <- diag(p)
CovMatrix
(0.5)^3
# a)
p <- 4
n <- 1000
sigma <- 1
CovMatrix <- diag(p)
for (i in 1:p) {
m <- matrix(0, p, p)
m <- m[abs(row(m) - col(m)) == i] <- (0.5)^i
CovMatrix <- CovMatrix + m
}
CovMatrix
p <- 4
n <- 1000
sigma <- 1
CovMatrix <- diag(p)
for (i in 2:p) {
m <- matrix(0, p, p)
m <- m[abs(row(m) - col(m)) == i] <- (0.5)^i
CovMatrix <- CovMatrix + m
}
CovMatrix
CovMatrix <- diag(p)
CovMatrix
m <- matrix(0, p, p)
m <- m[abs(row(m) - col(m)) == i] <- (0.5)
m
m <- matrix(0, p, p)
m <- m[abs(row(m) - col(m)) == 1] <- (0.5)
m
m <- matrix(0, p, p)
m
row(m)
m <- matrix(0, p, p)
m[abs(row(m) - col(m)) == 1] <- (0.5)
m
p <- 4
n <- 1000
sigma <- 1
CovMatrix <- diag(p)
for (i in 2:p) {
m <- matrix(0, p, p)
m[abs(row(m) - col(m)) == i] <- (0.5)^i
CovMatrix <- CovMatrix + m
}
CovMatrix
p <- 4
n <- 1000
sigma <- 1
CovMatrix <- diag(p)
for (i in 1:p) {
m <- matrix(0, p, p)
m[abs(row(m) - col(m)) == i] <- (0.5)^i
CovMatrix <- CovMatrix + m
}
CovMatrix
# Załaduj Dane
df <- read.table("seeds_dataset.txt",sep = " ",h=TRUE)
df
View(df)
df <- read.table("seeds_dataset.txt",sep = " ",h=TRUE)
# a)
glm(Class~area,data = df, family = "binomial")
m1 <- glm(Class~area,data = df, family = "binomial")
predict(m1,type = 'Class')
# a)
m1 <- glm(Class~area,data = df, family = "binomial")
predict(m1,type = 'response')
exp(m1$coefficients[names(m1$coefficients)=='area'])
m1$beta
m1$Beta
coef(m1)
b0 = coef(m1)[1]
b1 = coef(m1)[2]
b0
b1
x <- (ln(7/3) - b0)/b1
x <- (log(7/3) - b0)/b1
x
log(2)
# c)
m2 <- glm(Class~., data = df, family = "binomial")
m2_BIC <- stepAIC(m2, direction = "backward", k = log(n)) #BIC
m2_BIC
summary(m2_BIC)
X1 <- model.matrix(m2)
X2 <- model.matrix(m2_BIC)
n <- nrow(data)
p1 <- ncol(X1)
p2 <- ncol(X2)
SSE1 <- sum(m2$residuals^2)
SSE2 <- sum(m2_BIC$residuals^2)
Fstat <- (SSE1 - SSE2)/(p2 - p1) / (SSE2/(n - p2))
1-pf(Fstat,p2 - p1,n-p2)
anova(m2, m1, test="F")
sum_m2_BIC <-s ummary(m2_BIC)
1 - pchisq(sum_m2_BIC$null.deviance-sum_m2_BIC$deviance,sum_m2_BIC$df.null-sum_m2_BIC$df.residual)
sum_m2_BIC <- summary(m2_BIC)
1 - pchisq(sum_m2_BIC$null.deviance-sum_m2_BIC$deviance,sum_m2_BIC$df.null-sum_m2_BIC$df.residual)
T <- sum_m2_BIC$null.deviance - sum_m2_BIC$deviance
T
sum_m2_BIC$df.null - sum_m2_BIC$df.residual
K_alpha <- qchisq(0.95, sum_m2_BIC$df.null - sum_m2_BIC$df.residual)
K_alpha
df <- read.table("seeds_dataset.txt",sep = " ", header = TRUE)
# Oświadczam, że niniejsza praca stanowiąca podstawę do uznania osiągnięcia efektów uczenia
# się z przedmiotu Metody Statystyki Obliczeniowej została wykonana przeze mnie samodzielnie.
# Mikołaj Kida 276944
# Kolokwium 2
library(MASS)
library(glmnet)
library(dplyr)
#______________________________________________________________________________
# Zadanie 1)
# a)
p <- 200
n <- 1000
sigma <- 1
CovMatrix <- diag(p)
for (i in 1:p) {
m <- matrix(0, p, p)
m[abs(row(m) - col(m)) == i] <- (0.5)^i
CovMatrix <- CovMatrix + m
}
# Wygeneruj próbkę
x <- mvrnorm(n, mu = seq(0.01,5,0.025), Sigma = CovMatrix)
eps <- rnorm(n, 0, sigma)
beta <- c(rep(1, 10), rep(0, 190))
y <- x %*% beta + eps
# Utwórz ramkę danych
dane <- data.frame(y, x)
# Podziel dane na podzbiory
rows <- sample(nrow(dane),n/2)
treningowy <- dane[rows,]
walidacyjny <- dane[-rows,]
# c)
# Cross Validacja
cv_model <- cv.glmnet(x = data.matrix(treningowy[,-1]), y = treningowy[,1], alpha = 0.5)
# Dopasowany model
model <- glmnet(x = data.matrix(treningowy[,-1]), y = treningowy[,1], alpha = 0.5, lambda = cv_model$lambda.min)
cv_model <- cv.glmnet(x = data.matrix(walidacyjny[,-1]), y = walidacyjny[,1], alpha = 0.5)
cv_walidacyjny <- cv.glmnet(x = data.matrix(walidacyjny[,-1]), y = walidacyjny[,1], alpha = 0.5)
# Oświadczam, że niniejsza praca stanowiąca podstawę do uznania osiągnięcia efektów uczenia
# się z przedmiotu Metody Statystyki Obliczeniowej została wykonana przeze mnie samodzielnie.
# Mikołaj Kida 276944
# Kolokwium 2
library(MASS)
library(glmnet)
library(dplyr)
#______________________________________________________________________________
# Zadanie 1)
# a)
p <- 200
n <- 1000
sigma <- 1
CovMatrix <- diag(p)
for (i in 1:p) {
m <- matrix(0, p, p)
m[abs(row(m) - col(m)) == i] <- (0.5)^i
CovMatrix <- CovMatrix + m
}
# Wygeneruj próbkę
x <- mvrnorm(n, mu = seq(0.01,5,0.025), Sigma = CovMatrix)
eps <- rnorm(n, 0, sigma)
beta <- c(rep(1, 10), rep(0, 190))
y <- x %*% beta + eps
# Utwórz ramkę danych
dane <- data.frame(y, x)
# Podziel dane na podzbiory
rows <- sample(nrow(dane),n/2)
treningowy <- dane[rows,]
walidacyjny <- dane[-rows,]
# c)
# Cross Validacja
cv_treningowy <- cv.glmnet(x = data.matrix(treningowy[,-1]), y = treningowy[,1], alpha = 0.5)
# Dopasowany model
model <- glmnet(x = data.matrix(treningowy[,-1]), y = treningowy[,1], alpha = 0.5, lambda = cv_treningowy$lambda.min)
# d)
# Cross Validacja
cv_walidacyjny <- cv.glmnet(x = data.matrix(walidacyjny[,-1]), y = walidacyjny[,1], alpha = 0.5)
# Optymalne lambda
cv_walidacyjny$lambda.min
plot(cv_walidacyjny)
# Oświadczam, że niniejsza praca stanowiąca podstawę do uznania osiągnięcia efektów uczenia
# się z przedmiotu Metody Statystyki Obliczeniowej została wykonana przeze mnie samodzielnie.
# Mikołaj Kida 276944
# Kolokwium 2
library(MASS)
library(glmnet)
library(dplyr)
#______________________________________________________________________________
# Zadanie 1)
# a)
p <- 200
n <- 1000
sigma <- 1
CovMatrix <- diag(p)
for (i in 1:p) {
m <- matrix(0, p, p)
m[abs(row(m) - col(m)) == i] <- (0.5)^i
CovMatrix <- CovMatrix + m
}
# Wygeneruj próbkę
x <- mvrnorm(n, mu = seq(0.01,5,0.025), Sigma = CovMatrix)
eps <- rnorm(n, 0, sigma)
beta <- c(rep(1, 10), rep(0, 190))
y <- x %*% beta + eps
# Utwórz ramkę danych
dane <- data.frame(y, x)
# Podziel dane na podzbiory
rows <- sample(nrow(dane),n/2)
treningowy <- dane[rows,]
walidacyjny <- dane[-rows,]
# c)
# Cross Validacja
cv_treningowy <- cv.glmnet(x = data.matrix(treningowy[,-1]), y = treningowy[,1], alpha = 0.5)
# Dopasowany model
model <- glmnet(x = data.matrix(treningowy[,-1]), y = treningowy[,1], alpha = 0.5, lambda = cv_treningowy$lambda.min)
# d)
# Cross Validacja
cv_walidacyjny <- cv.glmnet(x = data.matrix(walidacyjny[,-1]), y = walidacyjny[,1], alpha = 0.5)
# Optymalne lambda
cv_walidacyjny$lambda.min
plot(cv_walidacyjny)
# e)
istotne <- sum(which(model$beta != 0) <= 10)
nieistotne <- sum(which(model$beta != 0) > 10)
istotne
niesistotne
nieistotne
# Procent zmiennych nieistotnych wybranych do modelu
nieistotne/190
# Procent zmienncyh istotnych wybranych do modelu
istotne/10
install.packages("quantmod")
dane <- read.table("DOWJ.DAT")
dane <- read.table("DOWJ.DAT")
install.packages("quantmod")dane
dane
# a)
dane_ts <- ts(data=dane)
# a)
dane_ts <- ts(data=dane, start="28-08-1972", end="18-12-1972")
dane_ts
dane
# a)
dane_ts <- ts(data=dane, start=ymd("1972-08-28"), end=ymd("1972-12-18"))
library(lubridate)
# a)
dane_ts <- ts(data=dane, start=ymd("1972-08-28"), end=ymd("1972-12-18"))
dane_ts
# b)
plot(dane_ts)
# a)
dane_ts <- ts(data = dane, start = decimal_date(ymd("1972-08-28")), end = decimal_date(ymd("1972-12-18")))
dane_ts <- ts(data = dane, start = decimal_date(ymd("1972-08-28")), end = decimal_date(ymd("1972-12-18")))
# b)
plot(dane_ts)
# a)
dane_ts <- ts(data = dane, start = ymd("1972-08-28"), end = ymd("1972-12-18"))
# b)
plot(dane_ts)
# a)
dane_ts <- ts(data = dane, start = ymd("1972-08-28"), frequency = 365)
# b)
plot(dane_ts)
# a)
dane_ts <- ts(data = dane, start = ymd("1972-08-28"), end = ymd("1972-12-18"))
# b)
plot(dane_ts)
dane
# a)
dane_ts <- ts(data = dane, start = decimal_date(ymd("1972-08-28")), frequency = 365)
# b)
plot(dane_ts)
# c)
acf(dane_ts)
# c)
acf(dane_ts)
# d)
diff(dane_ts)
# d)
dane_sz <- diff(dane_ts)
# e)
plot(dane_sz)
Box.test(dane_sz, lag = 10, type = "Ljung")
# f)
acf(dane_sz)
pacf(dane_sz)
# f)
acf(dane_sz)
pacf(dane_sz)
dane_sz[-nrow(dane_sz)]
sz_2 <- c(NA, dane_sz[-nrow(dane_sz)])
AR <- lm(dane_sz ~ sz_2)
AR$coefficients
sc_AR <- AR$coefficients[1] + AR$coefficients * sz_2
sc_AR <- AR$coefficients[1] + AR$coefficients[2] * sz_2
plot(sc_AR)
sc_AR <- ts(AR$coefficients[1] + AR$coefficients[2] * sz_2)
plot(sc_AR)
acf(sc_AR)
sc_AR
plot(AR$residuals)
acf(AR)
sz_2 <- c(NA, dane_sz[-nrow(dane_sz)])
AR <- lm(dane_sz ~ sz_2)
acf(AR$residuals)
MA <- arima(dane_sz, c(1,0,0))
acf(MA$residuals)
acf(AR$residuals)
MA <- arima(dane_sz, c(0,0,2))
acf(MA$residuals)
Box.test(MA, lag = 10, type = "Ljung")
Box.test(MA$residuals, lag = 10, type = "Ljung")
Box.test(AR$residuals, lag = 10, type = "Ljung")
library(shiny); source('C:/Users/KidziaK/Desktop/Analiza dźwięku/app.R')
getwd()
source('Analiza dźwięku/app.R')
source('AIPSD/app.R')
dane <- read.csv("follic_short.csv")
setwd("~/")
dane <- read.csv("follic_short.csv")
dane <- read.csv("follic_short_cr.csv")
dane
install.packages(c("KMsurv", "survMisc"))
library(shiny); source('Analiza-i-przetwarzanie-d-wi-ku/app.R')
source('app.R')
source('app.R')
source('app.R')
source('app.R')
source('app.R')
source('app.R')
source('app.R')
source('app.R')
source('app.R')
source('app.R')
source('app.R')
source('app.R')
